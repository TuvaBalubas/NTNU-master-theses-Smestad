\chapter{Discussion}
\label{chap:discussion}

The results you have collected and the process you when through to develop the project have been presented earlier.  This Chapter is used to talk about your interpretations of results or the process.  It might be a discussion of the language you used.  A tool that you started to use but then stopped using for some reason.  It could give insight into the evolution of your process.

Discussing the design process: what worked? what didn't - how could it improve? Agree framework e.g.other challenges in the design process? Chatfuel as a tool, how did it work where was it limited - answer research question 1 and sub questions - did it help?

\section{Discussion: The implementation of the personality framework}

The first research question asks \textit{How can we design chatbot personalities to guide the design process of a chatbot interface?}, and the sub-questions concerns whether personality can be used as a stable pattern to guide the design process, which elements must be considered to inform the personality, and how can we ensure that the personality meets user needs and expectations. Through following the personality framework, the researcher found that the elements that must be considered to inform the personality are as follows:

\begin{enumerate}
    \item A deep understanding of the users and their needs
    \item The brand mission, goals, and values
    \item The role/job of the chatbot
    \item An appropriate personality model
\end{enumerate}

These four elements were all used to form the personality of the chatbot, and helped narrow down the most important tasks the chatbot should perform, as well as understanding the way in which these tasks should be performed. As laid out in the methodology section, chatbots that are extensions of services provided by brands, must be consistent to the brand it represents. By having a deep understanding of the brand guidelines; tone of voice, values, mission and goals, will limit and set precise guidelines for the way in which the chatbot should behave. In addition to how it should conduct itself, it will also have to be built to understand how it should handle the different users regarding their emotions, needs, expectations, and contexts. This specific understanding not only adds to its behaviour, but also allows designers to plan for which characteristics should be present at what time. If the users are frustrated or in a hurry, the chatbot should not behave in a way that adds to their frustrations or become more time consuming. Understanding these helps write user scenarios, and will add to the knowledge of the chatbot, as different user scenarios needs different use cases. Therefore, designers have already predicted and planned for use cases that the chatbot might have to handle. Personality plays into this in regards to how it is designed to respond or act in these scenarios. The chatbot built for this thesis were built on the knowledge that its users will not respond well to a lecturing tone, the "I-know-best", and therefore took a much different approach to providing help and tips. The last point here also consists of the role of the chatbot, the job its performing again limits the way in which it should behave. This chatbot was defined not only as an assistant, but a motivator as well. It was there to support, guide, and help, not to lecture, challenge, and tell - which might be a suitable approach for a different context and a different target audience. This shows how important it is to not choose a personality before we know 1) who the users are 2) which values it represents 3) which job it is performing! This should form the basis of the personality, and not come after-the-fact. Then you need to find an appropriate personality model. 

This project chose to use the five-factor model to model the chatbot personality. This model was chosen over e.g. MBTI, HEXACO or Jung, and this is because the five factor model has been found to be suitable for multiple countries and cultures, and provides a descriptive taxonomy, in which organises natural language and the scientific traits into a single framework. It is therefore an organised and easy framework to use when wanting to build a personality. The model is created to assess persons personality types, but can very well be reversed to build a suitable personality; as done in this thesis project. Having the list of traits, organised under the five factors, was helpful to use throughout the scripting of the chatbot. It was a helpful tool when writing each use case, and plan for any conversation flow. It was also very helpful when building the personality for Chatbot B, as it offered an organised way to model an "opposite" personality. Other UX professionals have offered another view on how to model personality, which includes basing the personality on well known personalities; such as that of specific celebrities, historical figures, politicians, or fictional characters. This approach is a very creative take on writing personalities for chatbots, by combining and mixing traits from well-known personalities. For this context however, this approach would maybe have been useful in the ideation/exploratory phase of building the chatbot personality, and could provide a fun and collaborative way to design personalities in teams. There would however still be necessary to use a personality type model to organise the chosen personality after building on based on known personalities. 

The personalities given to Chatbot A and Chatbot B did not include any traits from the neuroticism factor, and there is a good explanation for why this is. In character development, it is emphasised that characters should have flaws and weaknesses in order for them to be seen as realistic and to give depth to the character. The chatbot personality does not include any intended flaws or weaknesses, and this is because of its role and context. It is crucial that the chatbot is a good representative for the internal and core values of the brand. This means that it needs to conduct itself the same way as other employees are expected to conduct themselves when representing the brand. The phrase "there are no bad days in customer service" also extends to the personality of the chatbot. The chatbot does not have "bad days", it does not experience sorrow, loss, or anger, it does not worry, get tired or frustrated - and more importantly it should not display any emotions or personality features that might be perceived negatively by the users. A chatbot in a different context, with a different role to play, might just be designed to appear flawed, and be influenced by emotions and events. However, in this context, such traits are not appropriate or desired.

The answer to Research Question 1, and its sub-questions, is the personality framework presented in this thesis; the personality framework includes the defined elements to be considered to inform the personality, it also includes user centred methods to ensure that they personality meets user need and expectations, and the researcher's experience implementing the framework found that it does provide a stable pattern to guide the design process. By always referring the choices of the design process back to the personality, helped write use cases and the conversation flow for the chatbot. It helped form the answers to specific use-cases, but also to plan for how the chatbot needed to handle specific scenarios. By knowing how the chatbot should behave, we can also to a greater certainty predict what the course of the conversation most likely will be. Knowing that the personality is appropriate to the users also made it easier to understand which behaviours that would not only be inconsistent with the personality, but also which behaviours that would be perceived more negatively by the users.

\section{Discussion: The experiment}
Before discussing the results from both parts of the experiment, let us first talk about the experiment design as laid out in the methodology section in regards to how this was implemented. It was the intention when buiding the prototype, to allow participants to interact freely with the chatbot; getting to know it and exploring all the different tasks it could perform. Unfortunately this was not possible to do as it was revealed later in the design process that the machine learning that forms the AI that Chatfuel delivers, had several flaws. As explained in the methodology chapter, it is very important that the chatbot does not fail to predict a user intent, or answers the wrong question during the experiment, as this affects how users perceive the chatbot overall. For the AttrakDiff measurement tool to give a true evaluation of the user experience, the pragmatic qualities, or the usability and usefulness factors, must be met to a high standard by the chatbot. It was not the intention to evaluate the usability of the chatbot in regards to its performance, but rather allow users to test the overall experience of interacting with the chatbot. It was therefore necessary to control the conversation by limiting the participants freedom in what they could ask the chatbots during the experiment. This way made it easier to only focus on building out eight conversation flows that the researcher was confident that the chatbot could handle without fail. A few of the participants did state that it was difficult to give a true assessment of the chatbots when they had only interacted with it through a short session, and that they were not able to ask it what they wanted and test it in a true setting. This however, did not seem to affect how they rated the two personalities, as they were all certain of which they preferred and what they liked or disliked in each of the chatbots. None of the participants knew that the research aim was to investigate the effects of the personalities, and therefore a lot of the feedback were in relation to its performance: "I would have liked to try it out more before evaluating it, as I don't know how it would perform in a real life setting". When participants were asked to provide commentary on the two chatbot versions, it was clear that the ammount of interaction was enough for them to offer a clear and precise evaluation of preference towards their personalities. 

The two-by-two factorial design was used to avoid an interaction effect on the results. It was assumed that participants might be more "careful" when rating the first chatbot version they interacted with, as they had no comparison to evaluate it against. Their opinion of the first chatbot might also influence how they evaluate the second chatbot. The results indicated that a majority of participants were less inclined to use the outer edges of the scales when evaluating the first chatbot, but used more often the outer edges when evaluation the second chatbot. A few of the participants stated during their second round of evaluation that they wished to go back and change their answers from the first round of evaluation as they felt they should have used more of the outer edges of the scales. All participants were of course not allowed to change their initial answers, and they were assured however that the final results took that into account, so they were not to worry. 

In addition to the AttrakDiff measurement tool to assess the user experience, each participants were also asked at the end of the experiment to answer which of the two chatbots they preferred. Twelve of the sixteen participants preferred Chatbot A over Chatbot B, three males and one female preferred Chatbot B. Although they stated to prefer Chatbot B, they rated Chatbot A higher in the AttrakDiff evaluation, but they rated Chatbot B a little higher in pragmatic qualities. The reasons they gave for preferring Chatbot B, was that they expected to grow tired of Chatbot A over time. They assumed that the same answers over and over again would become irritating and tiresome over time. This provides important information regarding the need to allow chatbots to adapt and grow over time. Conversational UX is explained as designing interfaces that learns from interacting with users. This means that not only does the chatbot need to learn the preferences and remember previous information of its users; it also needs to adapt its language over time. Some of the participants who preferred Chatbot B also admitted that they would be more interested in the "assistant" features, such as generating meal plans, recipes, grocery lists, and keeping track of they weekly shopping. They were more interested in these tasks being handled efficiently and effectively, as these tasks needs to be to the point. 


\subsection{Discussion of characteristics results}
The results of the degree participants perceived the personality characteristics of both chatbots showed that users did rate all factors for Chatbot A as more present than in Chatbot B. The paired samples t-test found this difference to be significant, and the two-way repeated measures ANOVA found no interaction effect of the starting condition or gender. This information tells us that the two chatbot personalities are seen as significantly different, with a significant higher perceive presence of characteristics in Chatbot A. To be able to say that the personalities were perceived as intended, Chatbot B would have had to receive lower scores than what it did in most of the characteristics. While the mean scores for all characteristics of Chatbot A were more than 4, and therefore supports $H_1 2$, Chatbot B received higher means than expected for Agreeableness (3,4219) and Openness (3,4531). This means that we cannot fully support $H_1 3$, and would have to keep the $H_0 3$. Although for Chatbot B to be perceived as intended would needed a lower score than what it was given, it was still rated as lower in the perceived characteristics compared to Chatbot A. Therefore, as it was significantly different and lower in scores compared to Chatbot A, it was suitable to use for comparison when testing the user experience in part two of the experiment.

Females and males both rated characteristics in Chatbot A as more present than the same characteristics in Chatbot B as can be seen in figure \ref{fig:perschargender}. Female means for Chatbot A and Chatbot B were slightly higher scored than the means for males. The male mean score for Extroversion for Chatbot A was higher than the mean score for females, and males scored Chatbot B higher in agreeableness than females. Apart from those small differences, each characteristic for each chatbot were rated more or less consistently between males and females. The conscientiousness factor was assumed to be rated more or less equal for both chatbots, and the mean scores differed by 0,375 showing that this factor was perceived almost the same for each chatbot by all participants. It is interesting though that Chatbot A was seen as somewhat more conscientious than Chatbot B, indicating that users found that version to be slightly more perceptive, reliant, and consistent than the other.  

As mentioned earlier, both personalities should have been tested in several iterations before conducting the second part of the experiment. This would have helped understanding exactly what the perceived personalities would be before testing the effect the personalities had on the user experience. However, the pilot testing were enough to satisfy that the two personalities were perceived as opposites to an extent, and the results confirmed this.


\subsection{Discussion of user experience results}

The figure \ref{fig:wordpairs} shows clearly where the differences are largest, such as human-technical, and that for the majority of word-pairs Chatbot A scores higher. The exceptions includes mainly word-pairs in the pragmatic quality: cumbersome-straightforward, unpredictable-predictable, unprofessional-professional. This suggests that a technical, impersonal or machine-like personality are seen as more professional and straightforward than a more human-like personality. The mean score for professional-unprofessional for Chatbot A was 5,875 and Chatbot B mean score was 6,25. This shows that a machine like and technical Chatbot B were perceived as conducting itself to a higher professional standard than Chatbot A, ever so slightly.

Although there was found no significant interaction effect caused by the starting condition, there are some minor interesting observations regarding the starting condition. Those who started with Chatbot A, group 1, rated it higher in hedonic quality stimulation and attractiveness than those who began with Chatbot B, group 2. Pragmatic quality and hedonic quality identity was rated higher by group 2 for Chatbot A. Group 2 rated Chatbot B more than a whole point higher for hedonic quality stimulation than group 1. Hedonic quality identity and attractiveness was also rated higher by group 2 than group 1, while the pragmatic quality was scored lower in group 2. All these differences are down to the decimals, except hedonic  quality stimulation for Chatbot B, but still offers some interesting observations regarding the impact of the starting condition for each group; e.g. both chatbots were rated higher in attractiveness during the first round, indicating that attractiveness was evaluated higher when participants had no comparison. The differences in Chatbot A between group 1 and group 2, were minor 0,25 at the most, while the differences in Chatbot B for each group were larger. This indicated that Chatbot A had a greater impact on the rating of Chatbot B, than Chatbot B had on Chatbot A.

\begin{table}[H]
\centering
\begin{tabular}{lll|ll}
\hline
& Group 1 &  & Group 2 &  \\
\hline
& Round 1 Chatbot A & Round 2 Chatbot B & Round 1 Chatbot B & Round 2 Chatbot A \\
\hline
PQ: & 5,9107 & 5,5536 & 5,375 & 5,9464 \\
HQ-I: & 5,4464 & 4,6607 & 4,9821 & 5,5179 \\
HQ-S: & 5,75 & 4,25 & 5,3929 & 5,5 \\
ATT: & 6,4107 & 5,0357 & 5,8214 & 6,2857 \\
\end{tabular}
\caption{Starting condition and mean scores for each groups}
    \label{tab:my_label}
\end{table}


\subsection{Regarding Facebook and the Cambridge Analytica scandal}

Because of the Facebook and Cambridge Analytica scandal during the spring of 2018; Facebook stopped reviewing any third party applications running through their platform. This resulted in the Chatbot B prototype not being able to be published through the messenger platform in time for the experiments. Chatbot A was able to be published through an existing application already reviewed by Facebook before the scandal, however the name of the chatbot was unable to be changed in time of the experiment.

\subsection{Possible confounders}
Because Facebook stopped reviewing new third party applications during the experiment, Chatbot B did not display a profile picture, but instead the chatfuel logo, and the name was displayed as "Chatfuel-test your bot", as this had to be tested through the unpublished test service provided by Chatfuel, rather than being run through its own page and account. Chatbot A however was runned through its own page and account, but the name of the chatbot belonged to a previous version of a chatbot displaying the wrong name. The absence of an image and name for Chatbot B could have impacted how users perceived this chatbot. Some unconsciously referred to this chatbot as a "he" rather than "she" for Chatbot A - even though all participants were instructed before beginning the tests that they are about to test two versions of the chatbot "Bella". The cards each participants used during the test had Bella's face on them in order to remind them of who they are talking to. 

Another possible confounder was one that the researcher failed to recognise before during the testing. This was the recipes suggested by the chatbot. A few of the test participants told the researcher after the experiment had finished, that they rated e.g. the word pair \textit{inventive-conventional} higher because the recipes in themselves were inventive - or they rated the characteristic helpful lower because they did not like the recipes suggested to them. The participants were not instructed to assess the personality of the chatbot, but the chatbot as a product in its entirety, therefore a few of the word-pairs assessed in the AttrakDiff measurement tool might be seen as negative for a product, but positive for a personality e.g. \textit{undemanding-challenging}, were \textit{challenging} was seen as a negative trait to describe the agent by some while for the assessment of a product it is seen as a positive trait. Another word-pair that confused participants were \textit{separates me from people - brings me closer to people}, as a lot of participants noted that talking to a robot would in the long run separate them from people rather than bringing them closer.

Building a personality framework would be useless, if personality did not have an improved effect on the user experience. This is why this project is twofold 1) building, implementing, and testing a personality framework for chatbot interfaces, 2) testing whether personality improves the user experience of chatbot interfaces. The second would be difficult to test without a personality framework, and the framework would be useless if the result of implementing it did not in any way affect the user experience.

\section{Contribution}

\section{Limitations}
Sample size

The implementation of the personality framework for this specific thesis project lacked enough iterations of testing. Time constraints affected how many times the modelled personality could be tweaked and improved through user testing, also the access to users made this difficult as testing the personality should be done by an appropriate sample belonging to the target group. Testing this on the participants throughout the design process would have biased them for the final experiment. Therefore initial testing of the personality, conducted through the design process used any person on hand that was willing to test the personalities. It would therefore have been more accurate if it could have been more thoroughly tested before testing the user experience of the personalities.

The use of Chatfuel to build the chatbot prototypes provided some challenges, and limitations. Chatfuel was chosen because it allowed for more dynamic conversation flows than other tools, and provided a simple interface to organise the different intents and user flows. The AI provided by chatfuel however were not as sophisticated as believed when choosing it as the bot builder platform. Previously, other platforms for building bots have been tested that provided much better AI. It appeared after a while that the machine learning Chatfuel uses were extremely flawed. It did not e.g. ignore stop words, and worked poorly to extract key words to separate two similar user intents. It also had issues with typos and upper and lower cases. Only after writing more than 300 unique phrases for each intent was it discovered that the machine learning failed to improve the AI to a desirable standard. Had this been noticed earlier, the chatbots would most likely been built using Recast.ai, which provides better AI and NLP capabilities. When the flawed AI of Chatfuel had been discovered, it was too late in the process to move to another platform, as Chatfuel did not allow any form of migration or export. Chatfuel does however provide, as stated earlier, a very organised and easy to use interface and they allowed for flexible linking of chatbot skills to create varied conversations, that other platforms lack. 

Being able to test the personalities over a longer period, to assess how the personality is perceived after users have become used to it, would add more to the effects of personality. It would have been interesting to see the long term effects, as well as providing knowledge in regards to how the personality needs to adapt to not become annoying, or evolve in the wrong directions.