\chapter{Conclusion}
\label{chap:conclusion}

This thesis aimed to investigate how we can design user centred chatbot personalities and whether personality matters for the user experience of chatbots. The latter has been a long standing assumption, and based on findings related to other interfaces than chatbots and CAs. While findings regarding traditional web and app user interfaces have found that personality is an important factor for how users perceive the interface, no similar study has yet investigated whether the same is true for chatbots. If personality does not matter, then we do not need to spend time, money and effort to create personalities for chatbots, and can spend more time on further developing and improving the AI and NLP capabilities, and offer a seamless experience with great and effective task handling. However, the findings presented in this thesis suggests that a human personality not only improves the user experience compared to chatbots without a human personality, it also impacts a user's perception overall. Even though Chatbot A and Chatbot B provided the same service, just as effectively and efficiently, Chatbot A was still rated higher than Chatbot B in the pragmatic qualities. This shows that an overall great user experience also improves how users perceive other qualities of the chatbot that are not in reality better than other versions. Building a framework to design personalities for conversational agents would be useless if personality did not have an improved effect on the user experience. This is why this project is twofold 1) building, implementing, and testing a personality framework for chatbot interfaces, 2) testing whether personality improves the user experience of chatbot interfaces.

The answer to Research Question 1, and its sub-questions, is the personality framework presented in this thesis; the personality framework includes the defined elements to be considered to inform the personality, it also includes user centred methods to ensure that the personality meets user needs and expectations. The researcher's experience implementing the framework found that it does provide a stable pattern to guide the design process of chatbot interfaces. By always referring the choices of the design process back to the personality, helped write use cases and the conversation flow for the chatbot. It helped form the answers to specific use-cases, such as handling a frustrated user or a happy one, and also to plan for how the chatbot needed to handle specific scenarios. By knowing how the chatbot should behave, we can also to a greater certainty predict what the course of the conversation most likely will be. Knowing that the personality is appropriate to the users also made it easier to understand which behaviours that would not only be inconsistent with the personality, but also which behaviours that would be perceived more negatively by the users. In addition to this, the framework also provided necessary information regarding the most suitable role the chatbot could take on to support users towards their goals. The chatbot role dictated which characteristics that were suitable, and helped understand the most valuable tasks and information it could provide users with.

The findings from the statistical analysis of the personality characteristics evaluation found a significant difference between the four factors (Extroversion, Agreeableness, Conscientiousness, Openness) for Chatbot A and Chatbot B. There was no significant interaction effect between the starting condition and gender. The results found that Chatbot A was rated higher on all factors than Chatbot B. We can therefore keep our research hypothesis $H_1 1$ that users will perceive the personality of Chatbot A as different to the personality of Chatbot B. As for $H_1 2$ and $H_1 3$ in that users will perceive both Chatbot A as intended and Chatbot B as intended, we can keep $H_1 2$ as it received a very high score on all factors. For $H_1 3$ however, the researcher expected a lower score for Chatbot B in the two factors: Openness and Agreeableness. The extroversion factor was perceived as intended as Chatbot B received a very low mean score of 2,2708. The conscientious factor was assumed to be rated more or less equal for both chatbots, and the mean scores differed by 0,375, confirming to some extent with the researcher's assumptions. We can conclude that Chatbot A was perceived as intended with high scores on all factors, while Chatbot B were perceived as intended in the Extroversion and Conscientiousness factors, and not for the Agreeable and Openness factors as the intention was for users to perceive them to a much lower degree. 

The findings from the statistical analysis of the results of the AttrakDiff data found there to be a significant difference between the pragmatic quality, hedonic quality and attractiveness of Chatbot A and Chatbot B. The two-way repeated measures ANOVA found a significant main effect between the two personalities with respect to the user experience. The results of the paired-samples t-test found that there was a significant improved effect on the user experience of Chatbot A compared to Chatbot B. Chatbot A was rated to a higher degree in all factors, pragmatic quality, hedonic quality and attractiveness, over Chatbot B. There was no significant interaction effect found between the starting condition with respect to the personality or user experience score, nor was there found a significant interaction effect of gender. Therefore, based on this analysis, we can keep our research hypothesis $H_1 4$ that personality affects the user experience of chatbots and $H_1 5$ that Chatbot A will have an improved effect over Chatbot B. 

\vspace{5mm}

\section{Future Research}
\label{sec:future}

For future research it is recommended to test the effects of the personality over a longer period of time. Participants commented that they could grow tired of the personality if being subjected to it over a longer period, especially if some answers were repeated over and over again. Therefore understanding the long-term effects is important to inform how we can design the chatbot to behave over time, and how it should behave dependent on the preferences of the specific user is important. Allowing the chatbot and the personality to learn and adapt to the specific user, is assumed to be preferred by users. By testing the modelled personalities over a longer period, to assess how the personality is perceived after users have become used to it, would add more to the effects of personality. It would have been interesting to see the long term effects, as well as providing knowledge in regards to how the personality needs to adapt to not become annoying, or how we can ensure that the personality does not evolve in the wrong direction.

IBM and others are currently using tone analysing to read the mood of the author of texts. This could be chat messages, social media post, articles etc. By implementing the tone analyser in a chatbot, will allow it to understand the mood of the user throughout the interaction. Adapting the chatbot responses, and the dynamics of its personality and behaviour, to the mood of the user can be benefited by the framework presented in this thesis. The personality traits can help dictate whether the variations of answers still are consistent with the chatbot personality, and can help plan and prepare for the different moods users bring to the interaction. Exploring this further to improve the personality framework can help benefit the user experience of chatbots.

Another aspect for future research is to adapt the AttrakDiff measurement tool to become more suitable for conversational user interfaces. In addition to this, having a tool to be able to assess and evaluate the user experience when the CA grows and learns in regards to the specific user, will become important as chatbots are rapidly becoming more and more popular and intelligent.

As for ensuring that users perceive the personality as intended by the designers of the CA, it is recommended to use the Agree! framework developed by \cite{Callejas2014}. This framework is built to evaluate a personality in iterations during the design process. Each interaction involves a change in attitude (e.g. unfriendly, neutral, friendly), and the framework assesses whether participants are in agreement in regards to the perceived personality of the CA. The Agree! tool "computes a very wide range of numerical coefficients that measure the similarity of the perceived and target personalities, the impact of the user personalities in their perceptions, and also the level of agreement between your users" \citep{Callejas2014}.The framework can be used to run a score-based evaluation or a tag-based evaluation. The score-based evaluation computes the similarity between the target and perceived personalities of different personality traits, e.g. the five factors. The tag-based evaluation allows participants to choose between traits/tags to describe the personality from a predefined set of traits/tags.

Another aspect that has received little attention in the research community in regards to how humans perceive conversational agents, is the effects of gender. The gender of the agent has been found to impact the nature of the conversation, and also what kind of attention the CA receives from the different users. Understanding these effects, and how we can use this knowledge requires more attention from the research community.

